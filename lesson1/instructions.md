# Постановка задачи

В этом уроке мы решим задачу регрессии при помощи нейронной сети. Мы будем
предсказывать значение некоторой простой функции (например, синуса или косинуса)
в некоторой точке $x$.

Иными словами, мы сделаем сеть, которая делает аппроксимацию некоторой
неизвестной (скрытой) зависимости $f(x)$.

Стоит отметить, что это можно сделать с любой точностью при помощи сигмоидной
нейронной сети для ограниченной функции $f(x)$ с конечным числом разрывов.

# Инструкции для решения
## Сделайте датасет

В этом уроке мы сделаем датасет сами. Для этого мы возьмем некоторую функцию $f$
(ограниченную, имеющую не больше, чем счетное количество разрывов), и сделаем
набор пар $x_i, f(x_i) + \epsilon_i$, где $\epsilon_i$ -- некоторый шум, который
мы будем генерировать сами. Причина, по которой мы добавляем шум в наши данные
заключается в том, что всегда, когда мы производим измерения, мы получаем
значение с некоторой точностью.

В реальной жизни у нас будет просто набор пар $x_i, y_i$, в качестве датасета,
по которым необходимо восстановить скрытую зависимость $f$ такую, что
$$y_i = f(x_i) + \epsilon_i$$.

Для выполнения этого задания создайте файл sine_dataset.py и
1) сделайте в нем два класса: ```DataSetIndex``` и ```DataSet```

2) для класса ```DataSetIndex``` сделайте метод
```__init__(self, seed=0, n_items=1000, low=-10.0, high=10.0, noise=0.1)```.
Здесь инициализируйте генератор случайных чисел numpy значением seed,
создайте случайный набор координат, равномерно распределенных от ```low``` до
```high```,
посчитайте в этих координатах значение функции $sin(x)$, сгенерируйте
вектор шума соответствующего размера (нормальный шум со матожиданием 0 и
стандартным отклонением ```noise_level```), добавьте шум к значениям функции,
разбейте координаты и значения на три куска: train, valid и test (valid и test
должны составлять по 10% от всей выборки).
Сделайте переменную order для каждого куска датасета. Для train сделайте
перемешивание индексов, для valid и test можете оставить индексы в
исходном порядке.

3) для класса ```DataSetIndex``` сделайте метод
```shuffle(self)```. Этот метод будет вызываться после каждой эпохи и
перемешивать тренировочную выборку для дальнейшего обучения.

4) для класса ```DataSet``` сделайте метод
```__init__(self, ds_index, mode='train')```. В нем сохраните ds_index и mode
в обьект ```self``` класса ```DataSet```.

5) для класса ```DataSet``` сделайте метод ```__len__(self)```, который будет
возвращать количество примеров в ```self```. Помните, что количество примеров
разное для ```train```, ```valid``` и ```test``` кусков датасета.

6) для класса ```DataSet``` сделайте метод ```__getitem__(self, index)```,
который вернет два списка: список входных значений и список целевых значений
выходов. В этой задаче в каждом списке должно быть по одному числу.
Помните, что количество примеров
разное для ```train```, ```valid``` и ```test``` кусков датасета.

7) протестируйте датасет. Посмотрите, что возвращает DataSet при индексации
(например, при вытаскивании элемента с номером 0). Проверьте, что DataSet
работает во всех режимах (```train```, ```valid```, ```test```) для всех
индексов. Посмотрите, как выглядят наши данные: сделайте график при помощи
Jupyter Notebook значений $x$ и $f(x)$.

## Create a Model / Сделайте модель

Мы уже сделали скрипт датасета -- это половина работы. Вторая половина --
определиться с архитектурой модели, лосс-функцией, метриками, оптимизатором.

1) Для начала сделаем архитектуру модели. Для этого определим класс ```Model``` в
```model.py```. Он должен быть унаследован от класса ```torch.nn.Module```.

2) В этом классе сделайте функцию ```__init__```, в которой будут определены все
элементы архитектуры. В нашем случае нейронная сеть будет состоять из двух
линейных слоев (скрытого и выходного). Нужно создать эти слои в __init__.
Количество скрытых нейронов можно сделать параметром сети.

3) Далее, нужно определить, как будет вычисляться результат работы сети.
Создайте метод forward в классе Model. Этот метод принимает один аргумент
(список входов в сеть) и возвращает один результат (список выходов из сети).
Производите вычисления следующим образом:
вход -> скрытый слой -> сигмоида -> выходной слой.

Таким образом, мы сделали нашу нейронную сеть.

### Сделайте обработчик модели (Socket)

В этом классе находится вся информация о том, как взаимодействовать с моделью.

В этом классе нужно определить следующие методы: ```__init__``` -- конструктор,
```criterion``` -- лосс-функция, ```metrics``` -- метрики. Остальное определять
в этом задании не обязательно.

1) Сделайте метод ```__init__(self, model)```: сохраните модель в поле self.model,
определите тренируемые слои (поле ```self.train_modules```, в этом задании это вся
модель, поэтому можно в качестве тренируемых слоев передать ```self.model```),
задайте оптимизатор self.optimizer (пусть в нашем случае это будет Adam со
скоростью обучения ```3.0e-4```).

2) Сделайте метод ```criterion(self, output, target)```. Сюда будут приходить
списки результатов работы модели и целевых значений. В нашем случае эти списки
будут содержать по одному тензору. В нашем случае функцией ошибки будет
среднеквадратичное отклонение (Mean Squared Error, MSE). Используя операции
пакета torch расчитайте значение MSE между output[0] и target[0] и верните
результат.

3) Сделайте метод ```metrics(self, output, target)```. Сюда будут приходить точно
такие же списки, как и в метод ```criterion```. В этом задании мы будем
использовать метрику "доля точек с ошибкой меньше заданной":
$$ E_\epsilon = \frac{1}{N} \sum_{i = 1}^N \[|\hat{f}(x_i) - y_i| < \epsilon\].$$
Здесь $\hat{f}(x_i)$ -- результат работы нейронной сети в точке $x_i$, $y_i$ --
таргетное значение. Возьмите $\epsilon = 0.3, 0.1, 0.03, 0.01$. Это и будут
наши метрики. Верните словарь из метрик.
