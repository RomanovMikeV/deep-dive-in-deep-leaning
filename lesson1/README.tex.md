# Language

[English](#english)

[Русский](#russian)

<a name="english"/>

<a name="russian"/>

# Инструкции

[Постановка задачи](#rus_statement)

[Написание кода](#rus_solution)

  * [Датасет](#rus_dataset)
  
  * [Модель](#rus_model)
  
  * [Обработчик модели](#rus_socket)

[Тренировка модели](#rus_training)

[Помощь](#rus_help)

## Постановка задачи <a name="rus_statement"/>

В этом уроке мы решим задачу регрессии при помощи нейронной сети. Мы будем
предсказывать значение некоторой простой функции (например, синуса или косинуса)
в некоторой точке $x$.

Иными словами, мы сделаем сеть, которая делает аппроксимацию некоторой
неизвестной (скрытой) зависимости $f(x)$.

Стоит отметить, что это можно сделать с любой точностью при помощи сигмоидной
нейронной сети (функции активации -- сигмоиды) для ограниченной функции $f(x)$ с конечным числом разрывов.


## Написание кода <a name="rus_solution"/>

### Create a Dataset / Сделайте датасет <a name="rus_dataset"/>

В этом уроке мы сделаем датасет сами. Для этого мы возьмем некоторую функцию $f$
(ограниченную, имеющую не больше, чем счетное количество разрывов), и сделаем
набор пар $x_i, f(x_i) + \epsilon_i$, где $\epsilon_i$ -- некоторый шум, который
мы будем генерировать сами. Причина, по которой мы добавляем шум в наши данные
заключается в том, что всегда, когда мы производим измерения, мы получаем
значение с некоторой точностью. Мы рассмотрим случай, когда в каждой точке $x_i$ шум независим от значений в других точках и одинаково распределен.

В реальной жизни у нас будет просто набор пар $x_i, y_i$, в качестве датасета,
по которым необходимо восстановить скрытую зависимость $f$ такую, что
$$y_i = f(x_i) + \epsilon_i$$.

Для выполнения этого задания создайте файл sine_dataset.py и
1) сделайте в нем два класса: ```DataSetIndex``` и ```DataSet```

2) для класса ```DataSetIndex``` сделайте
метод
```__init__(self, seed=0, n_items=1000, low=-10.0, high=10.0, noise=0.1)```.

* Здесь инициализируйте генератор случайных чисел numpy значением ```seed```

* создайте случайный набор координат, равномерно распределенных от ```low``` до
```high```

* посчитайте в этих координатах значение функции $sin(x)$

* сгенерируйте
вектор шума соответствующего размера (нормальный шум со матожиданием 0 и
стандартным отклонением ```noise_level```)

* добавьте шум к значениям функции

* разбейте координаты и значения на три куска: ```train```, ```valid``` и ```test``` (```valid``` и ```test```
должны составлять соответственно по 10% и 50% от всей выборки).

* Сделайте переменную ```order``` для каждого куска датасета. В ней будет храниться порядок следования элементов датасета. Для ```train``` сделайте
случайный порядок индексов (при помощи ```numpy.random.permutation```), для ```valid``` и ```test``` сделайте порядок от 0 до N (```numpy.arange```).

3) для класса ```DataSet``` сделайте метод
```__init__(self, ds_index, mode='train')```. В нем сохраните ```ds_index``` и ```mode```
в обьект ```self``` класса ```DataSet```.

4) для класса ```DataSet``` сделайте метод ```__len__(self)```, который будет
возвращать количество примеров в ```self```. Помните, что количество примеров
разное для ```train```, ```valid``` и ```test``` кусков датасета.

5) для класса ```DataSet``` сделайте метод ```__getitem__(self, index)```,
который вернет два списка: входное значение для нашей сети, обернутое в список, и целевое значение для выходов, оберутое в список.

6) для класса ```DataSet``` сделайте метод
```shuffle(self)```. Этот метод будет вызываться в начале каждой эпохи (эпоха -- период, за который нейросеть просматривает по разу все обучающие примеры) и
перемешивать тренировочную выборку для эпохи. Валидационную и тестовую выборку он перемешивать не должен.

7) протестируйте датасет. Посмотрите, что возвращает ```DataSet``` при индексации
(например, при вытаскивании элемента с номером 0). Проверьте, что ```DataSet```
работает во всех режимах (```train```, ```valid```, ```test```) для всех
индексов. Посмотрите, как выглядят наши данные: сделайте график при помощи
Jupyter Notebook значений $x$ и $f(x)$.

### Сделайте модель <a name="rus_model"/>

Мы уже сделали скрипт датасета -- это половина работы. Вторая половина --
определиться с архитектурой модели, лосс-функцией, метриками, оптимизатором.

1) Для начала сделаем архитектуру модели. Для этого определим класс ```Model``` в
```model.py```. Он должен быть унаследован от класса ```torch.nn.Module```.

2) В этом классе сделайте функцию ```__init__(self)```, в которой будут определены все
элементы архитектуры. В нашем случае нейронная сеть будет состоять из двух
линейных слоев (скрытого и выходного). Нужно создать эти слои в __init__.
Количество скрытых нейронов можно сделать параметром сети.

3) Далее, нужно определить, как будет вычисляться результат работы сети.
Создайте метод ```forward``` в классе ```Model```. Этот метод принимает один аргумент
(список входов в сеть) и возвращает список результатов (список из одного тензора в нашем случае).
Производите вычисления следующим образом:
вход -> скрытый слой -> сигмоида -> выходной слой.

Таким образом, мы сделали нашу нейронную сеть.

### Сделайте обработчик модели (Socket) <a name="rus_socket"/>

В этом классе находится вся информация о том, как взаимодействовать с моделью.

В этом классе нужно определить следующие методы: ```__init__``` -- конструктор,
```criterion``` -- лосс-функция, ```metrics``` -- метрики. Остальное определять
в этом задании не обязательно.

1) Сделайте метод ```__init__(self, model)```: сохраните модель в поле self.model,
определите тренируемые слои (поле ```self.train_modules```, в этом задании это вся
модель, поэтому можно в качестве тренируемых слоев передать ```self.model```),
задайте оптимизатор self.optimizer (пусть в нашем случае это будет Adam со
скоростью обучения ```3.0e-4```).

2) Сделайте метод ```criterion(self, output, target)```. Сюда будут приходить
списки результатов работы модели и целевых значений. В нашем случае эти списки
будут содержать по одному тензору. В нашем случае функцией ошибки будет
среднеквадратичное отклонение (Mean Squared Error, MSE). Используя операции
пакета torch расчитайте значение MSE между output[0] и target[0] и верните
результат.

3) Сделайте метод ```metrics(self, output, target)```. Сюда будут приходить точно
такие же списки, как и в метод ```criterion```. В этом задании мы будем
использовать метрику "доля точек с ошибкой меньше заданной":

$$ E_\epsilon = \frac{1}{N} \sum_{i = 1}^N \[|\hat{f}(x_i) - y_i| < \epsilon\].$$

Здесь $\hat{f}(x_i)$ -- результат работы нейронной сети в точке $x_i$, $y_i$ --
таргетное значение. Возьмите $\epsilon = 0.3, 0.1, 0.03, 0.01$. Это и будут
наши метрики. Верните словарь из метрик.

## Тренировка модели <a name="rus_training"/>

1) Запустите процесс тренировки
```
scorch-train --model model.py --dataset dataset.py --epochs 10 -cp 10_epochs
```

2) Также сделайте обучение на протяжении 100 эпох
```
scorch-train --model model.py --dataset dataset.py --epochs 100 -cp 100_epochs
```

3) Оцените визульно качество обучения:

* сделайте тестовый прогон модели с 10 эпохами
  ```
  scorch-test --model model.py --dataset dataset.py -cp 10_epochs --prefix 10_epochs
  ```

* cделайте тестовый прогон модели со 100 эпохами
  ```
  scorch-test --model model.py --dataset dataset.py -cp 100_epochs --prefix 100_epochs
  ```

4) считайте результат из ноутбука и сделайте график получившейся функции

## Помощь <a name="rus_help"/>

Пример решения задачи можно найти в файле ```Solution.ipynb.pdf```
